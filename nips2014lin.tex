%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Jacobs Landscape Poster
% LaTeX Template
% Version 1.1 (14/06/14)
%
% Created by:
% Computational Physics and Biophysics Group, Jacobs University
% https://teamwork.jacobs-university.de:8443/confluence/display/CoPandBiG/LaTeX+Poster
% 
% Further modified by:
% Nathaniel Johnston (nathaniel@njohnston.ca)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%------------------------------------------------------------------------------

\documentclass[final]{beamer}

\usepackage[scale=1.15]{beamerposter} % Use the beamerposter package for laying out the poster
\usepackage{subfig}
\usepackage{multirow}

\usetheme{confposter} % Use the confposter theme supplied with this template



\setbeamercolor{block title}{fg=ngreen!83,bg=white} % Colors of the block titles
\setbeamercolor{block body}{fg=black,bg=white} % Colors of the body of blocks
\setbeamercolor{block alerted title}{fg=white,bg=ngreen!70} % Colors of the highlighted block titles
\setbeamercolor{block alerted body}{fg=black,bg=ngreen!10} % Colors of the body of highlighted blocks
% Many more colors are available for use in beamerthemeconfposter.sty

% Use more stuff from tikz
\usetikzlibrary{arrows, backgrounds, positioning, fit}
\newcommand{\arclen}     {1.5cm}
\newcommand{\nodesize}   {1.8cm}
\newcommand{\nodepad}    {5pt}
\newcommand{\platepad}   {13pt}
\newcommand{\arcwidth}   {3pt}
\newcommand{\fieldwidth} {6pt}
\newcommand{\pointsize}  {4mm}

%-----------------------------------------------------------
% Define the column widths and overall poster size
% To set effective sepwid, onecolwid and twocolwid values, first choose how many columns you want and how much separation you want between columns
% In this template, the separation width chosen is 0.024 of the paper width and a 4-column layout
% onecolwid should therefore be (1-(# of columns+1)*sepwid)/# of columns e.g. (1-(4+1)*0.024)/4 = 0.22
% Set twocolwid to be (2*onecolwid)+sepwid = 0.464
% Set threecolwid to be (3*onecolwid)+2*sepwid = 0.708

\newlength{\sepwid}
\newlength{\onecolwid}
\newlength{\twocolwid}
\newlength{\threecolwid}
\setlength{\paperwidth}{48in} % A0 width: 46.8in
\setlength{\paperheight}{36in} % A0 height: 33.1in
\setlength{\sepwid}{0.024\paperwidth} % Separation width (white space) between columns
\setlength{\onecolwid}{0.22\paperwidth} % Width of one column
\setlength{\twocolwid}{0.464\paperwidth} % Width of two columns
\setlength{\threecolwid}{0.708\paperwidth} % Width of three columns
\setlength{\topmargin}{-0.5in} % Reduce the top margin size
%-----------------------------------------------------------

\usepackage{graphicx}  % Required for including images
\usepackage{booktabs} % Top and bottom rules for tables

% Variables and math notation
\input{notation}

% Red underline
\newsavebox\MBox
\newcommand\Cline[2][red]{{\sbox\MBox{$#2$}%
  \rlap{\usebox\MBox}\color{#1}\rule[-1.6\dp\MBox]{\wd\MBox}{1.5pt}}}


%------------------------------------------------------------------------------
%	TITLE SECTION 
%------------------------------------------------------------------------------

\title{Extended and Unscented Gaussian Processes} % Poster title

% \author{Daniel M.\ Steinberg\textsuperscript{1} 
%        \texttt{\small(daniel.steinberg@nicta.com.au)} and
%        Edwin V.\ Bonilla\textsuperscript{2}
%        \texttt{\small(e.bonilla@unsw.edu.au)}} % Author(s)
%
%\institute{\textsuperscript{1}NICTA, 
%           \textsuperscript{2}The University of New South Wales} % Institution(s)

\author{Daniel M.\ Steinberg
 \qquad \qquad    
       Edwin V.\ Bonilla \\
   \texttt{\small{daniel.steinberg@nicta.com.au}} \hspace{4.7cm}
        \texttt{\small{e.bonilla@unsw.edu.au}}
} % Author(s)

\institute{\vspace{-0.5cm}
 \hspace{4.5cm} NICTA  \hspace{7cm} The University of New South Wales
} % Institution(s)

%------------------------------------------------------------------------------

\begin{document}

\addtobeamertemplate{block end}{}{\vspace*{2ex}} % White space under blocks
\addtobeamertemplate{block alerted end}{}{\vspace*{2ex}} % White space under highlighted (alert) blocks

%\setlength{\belowcaptionskip}{2ex} % White space under figures
%\setlength\belowdisplayshortskip{2ex} % White space under equations

\begin{frame}[t] % The whole poster is enclosed in one beamer frame

\begin{columns}[t] % The whole poster consists of three major columns, the second of which is split into two columns twice - the [t] option aligns each column's content to the top

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % The first column


%------------------------------------------------------------------------------
%	Problem statement and aims
%------------------------------------------------------------------------------

\begin{alertblock}{Inverse Problems}
In many problems in science and engineering we have access to a \textbf{forward} or system
model, $\nonlin{\cdot}$:
\begin{equation*}
    \obs = \nonlin{\lstate} + \boldsymbol\epsilon
\end{equation*}
\vspace{-2cm}
\begin{itemize}
    \item We can measure the outputs of the system, $\obs$, but the inputs,
        $\lstate$, are \textbf{latent}.
    \item We wish to infer these inputs \emph{without} access to the inverse
        system model, $\nonlininv{\cdot}$.
    \item $\obs$ may be a continuous process or path (robot arm motion), 
        so $\lstate$ can be a  \textbf{Gaussian process} (GP).
\end{itemize}

\end{alertblock}


\begin{block}{Aims}

\begin{enumerate}
    \item Compute a posterior distribution over $\lstate$.
    \item Avoid  `hand-coding' methods for every new $\nonlin{\cdot}$, 
    i.e.~generic inference for non-linear likelihoods.
    \item The gradients, $\partial{\nonlin{\lstate}} /
        \partial\lstate$, may not be known.
    \item Avoid expensive simulations (cf.~MCMC).
\end{enumerate}

\end{block}


%------------------------------------------------------------------------------
%	INTRODUCTION
%------------------------------------------------------------------------------

\begin{block}{GPs with nonlinear likelihoods}

\textbf{Prior} on latent functions $\lstate$ at locations $\Inobs =
\cbrac{\inobs_n}^N_{n=1}$,
\begin{equation}
    \prob{\lstate} = \gausC{\lstate}{\prmean, \KERNL} \text{,}
\end{equation}
where $\prmean, \KERNL$ evaluate the mean function $\mu(\inobs)$ and 
 the covariance function 
$\kernl\!\brac{\inobs,  \inobs' ; \khypers}$ at all observed locations.

\vspace{1cm}

\textbf{Likelihood} encodes $\obs = \nonlin{\lstate} + \text{noise}$,
\begin{equation}
%    \probC{\obs}{\lstate}
 %   = \gausC{\obs}{\Cline{\nonlin{\lstate}}, \lvar\ident{N}}
%    = \prod_{n=1}^N 
%        \gausC{\obss_n}{\Cline{\nonlin{\lstates_n}}, \lvar}
    \probC{\obs}{\lstate}
   = \gausC{\obs}{\Cline{\nonlin{\lstate}}, \lcov}
 % =  \gausC{\obs}{\Cline{\nonlin{\lstate}}, \lvar\ident{N}}    
    = \prod_{n=1}^N 
        \gausC{\obss_n}{\Cline{\nonlin{\lstates_n}}, \lvar} 
 \text{.}
    \label{eq:like}
\end{equation}%

\vspace{1cm}
%
The \textbf{posterior} is the solution to our inverse problem:
\begin{equation}
     \probC{\lstate}{\obs} \propto {\probC{\obs}{\lstate}\prob{\lstate}}   \text{,}
    \label{eq:post}
\end{equation}
which is generally \emph{intractable} due to  the \emph{non-linear} nature of  $\nonlin{\lstate}$.
%\begin{equation}
 %   \prob{\obs} = \int \gausC{\obs}{\Cline{\nonlin{\lstate}}, \lvar\ident{N}}
 %       \gausC{\lstate}{\mathbf{0}, \KERNL} d\lstate
%\end{equation}

\end{block}


%------------------------------------------------------------------------------
%	Nonlinear Gaussian Models
%------------------------------------------------------------------------------

%\vspace{-1.5cm}
\begin{figure}
    %\subfloat[][]{
        %\vspace{-2cm}
        %\input{fig/gausmod.tex}
        %\label{sfig:gaus}
    %}
    %\subfloat[][]{
        %\input{fig/gp.tex}
        %\label{sfig:gp}
    %}
    %\caption{Graphical model of nonlinear Gaussian \subref{sfig:gaus} and
        %Gaussian process \subref{sfig:gp} inversion problems.}

    \input{fig/gp.tex}
    \label{fig:gp}

    \caption{A Gaussian process for inversion problems -- the mapping from 
          $\lstates_n$ to $\obss_n$ is given by the nonlinear
        forward model $\nonlin{\lstates_n}$.}
\end{figure}


%----------------------------------------------------------------------------------------

\end{column} % End of the first column




\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\twocolwid} % Begin a column which is two columns wide (column 2)

\begin{columns}[t,totalwidth=\twocolwid] % Split up the two columns wide column

\begin{column}{\onecolwid}\vspace{-.6in} % The first column within column 2 (column 2.1)


%------------------------------------------------------------------------------
%	Inference and Linearization
%------------------------------------------------------------------------------

\begin{block}{Variational Inference}
We approximate 
%\begin{equation}
$\probC{\lstate}{\obs} \approx \qrob{\lstate} = \gausC{\lstate} {\pomean,\pocov}$,
%\end{equation}
via the maximisation of the evidence \emph{lower bound}: 
\begin{equation}
    \Fengy = \expec{q\lstate}{\log \probC{\obs}{\lstate}}
        - \KL{\qrob{\lstate}}{\prob{\lstate}} \text{.}
    \label{eq:fengy}
\end{equation}%
%Then we use $\Fengy$ to \emph{optimise} posterior parameters:
%\begin{equation}
 %   \pomean^* = \argmax_\pomean{\Fengy}, \qquad
  %  \pocov^* = \argmax_\pocov{\Fengy},
%\end{equation}
% where $\qrob{\lstate}$ should approach the true posterior $\probC{\lstate}{\obs}$. 
% wrt the posterior parameters $\pomean$ and $\pocov$. \\
\vspace{0.5cm}%
\structure{\textbf{Main difficulty:}} `Intractable' expected log likelihood:
\begin{equation*}
    \expec{q\lstate}{\log \probC{\obs}{\lstate}} = 
    -\frac{1}{2\lvar} \expec{q\lstate}{
        \brac{\obs - \Cline{\nonlin{\lstate}}}\transpose
        \brac{\obs - \Cline{\nonlin{\lstate}}}} + \ldots
\end{equation*}
%We use two tricks to get around this problem;
\structure{\textbf{Our Solution:}} 
\begin{enumerate}
    \item \textbf{Linearize} the forward model: %$\nonlin{\lstate}$,
        \begin{equation}      
       \nonlin{\lstate} \approx  \nonlintilde{\lstate} = \Linmat\lstate + \intcpt  \text{,}
           \label{eq:linearise}
       \end{equation}
  and obtain linearised objective $\tilde\Fengy$.
    \item \textbf{Newton method} on  $\tilde\Fengy$ to
        find~$\pomean$,
%        \begin{equation}
%            \pomean_{k+1} = \pomean_k -
%            \step\brac{\nabla_\pomean\nabla_\pomean\tilde\Fengy}\inv 
%                \nabla_\pomean\tilde\Fengy \text{.}
%            \label{eq:newton}
%        \end{equation}
%       \item
       and `closed-form' updates for  $\pocov$:
\begin{align*}
 \pomean_{k+1} &= \brac{1-\step}\pomean_k + \step\prmean 
        + \step\Kgain_k\brac{\obs - \intcpt_k - \Linmat_k\prmean} \text{,} \\
    \pocov  &= \brac{\ident{N} - \Kgain_k\Linmat_k}\!\prcov,   
  \end{align*}        
where     $ \Kgain_k = \prcov\Linmat_k\transpose\brac{\lcov +
        \Linmat_k\prcov\Linmat_k\transpose}\inv$ .  
 \item
\textbf{Methods:} \emph{How to linearise $\rightarrow$ EGP vs.~UGP.}
\end{enumerate}
%
%Posterion parameters:
%\begin{align}
    %\pomean_{k+1} &= \brac{1-\step}\pomean_k + \step\prmean 
        %+ \step\Kgain_k\brac{\obs - \intcpt_k - \Linmat_k\prmean},
        %\label{eq:pomean} \\
    %\Kgain_k &= \prcov\Linmat_k\transpose\brac{\lcov +
        %\Linmat_k\prcov\Linmat_k\transpose}\inv, 
        %\label{eq:kgain} \\
    %\pocov &= \brac{\ident{D} - \Kgain\Linmat}\!\prcov
    %\label{eq:pocov}
%\end{align}%
\end{block}



%----------------------------------------------------------------------------------------

\end{column} % End of column 2.1

\begin{column}{\onecolwid}\vspace{-.6in} % The second column within column 2 (column 2.2)

%----------------------------------------------------------------------------------------
%	METHODS
%----------------------------------------------------------------------------------------


\begin{block}{Extended Gaussian Process (EGP)}
    
\begin{figure}
    \includegraphics[height=14cm]{fig/taylor_gp}
    \hspace{3cm}
%    \caption{Demonstration of linearising the forward model, $\nonlin{\cdot}$,
%        using a first order Taylor series expansion.}
\caption{EGP linearises  $\nonlin{\cdot}$ using a 1st-order Taylor  expansion.}
\end{figure}        

First order Taylor series expansion about $\pomean_k$,
\begin{equation*}
    \nonlin{\lstates_n} \approx \nonlin{\pomeans_{nk}} +
        \jacobs{\pomeans_{nk}}
        \brac{\lstates_n - \pomeans_{nk}},
\end{equation*}
and $\jacobs{\pomeans_{nk}} = 
    \partial\nonlin{\pomeans_{nk}}\!/\partial\pomeans_{nk}$. Then equating 
coefficients,
\begin{align}
    \Linmat_k &= \diag{\sbrac{\jacobs{\pomeans_{0k}}, \ldots,
        \jacobs{\pomeans_{Nk}}}}, \\
    \intcpt_k &= \sbrac{\nonlin{\pomeans_{0k}} - 
            \jacobs{\pomeans_{0k}}\pomeans_{0k}, \ldots,
            \nonlin{\pomeans_{Nk}} - \jacobs{\pomeans_{Nk}}\pomeans_{Nk}}
            \transpose. \nonumber
\end{align}
This results in the \emph{Gauss Newton} method.

\end{block}



%----------------------------------------------------------------------------------------

\end{column} % End of column 2.2

\end{columns} % End of the split of column 2 - any content after this will now take up 2 columns width

%----------------------------------------------------------------------------------------
%	IMPORTANT RESULT
%----------------------------------------------------------------------------------------

\begin{alertblock}{Key Results}

\begin{itemize}

    \item UGP %does not require any gradients
       % $\partial\nonlin{\lstate}\!/\partial\lstate$ to learn the posterior.
       treats the likelihood as a `black box' by not requiring knowledge of its form or its derivatives.

    \item $\Linmat$ is a \emph{diagonal} matrix because of the factorising
        likelihood in \eqref{eq:like} -- so similar complexity as Laplace
        approx.

    \item %We can also derive the iterative extended and sigma-point Kalman
        %filters using this framework.
        The iterative extended and sigma-point Kalman filters are specific instances of 
        our variational framework.
\end{itemize}

\end{alertblock} 

%------------------------------------------------------------------------------

\vspace{-1.5cm}


\begin{columns}[t,totalwidth=\twocolwid] % Split up the two columns wide column again
%
%
%
%\begin{column}{\onecolwid} % the first column within column 2 (column 2.1)
\begin{column}{\twocolwid}
%
%
\begin{block}{Experiments}
%
\begin{columns}
\begin{column}{0.6\twocolwid}%\vspace{-1.8in}

%\vspace{-2cm}
\structure{\textbf{Synthetic inversion problems}}


\begin{table}[tb]
    \centering

    \caption[]{
%    The EGP, UGP and a competing method~\cite{Opper2009} with
 %       various differentiable forward functions in the likelihood. The aim is
  %      to estimate the `true' latent function, $\lstate$, from noisy
  %      transformed observations, $\obs$, only given the forward
    %    transformation $\nonlin{\cdot}$. The UGP and EGP have similar
     %   perfomance, and outperform \cite{Opper2009}
 %
        Performance on synthetic inversion problems.
        }
    \vspace{5mm}
    \small
    \begin{tabular}{r|c| c c c c c c}
        \multirow{2}{*}{$\nonlin{\lstate}$} & \multirow{2}{*}{Algorithm} & 
            \multicolumn{2}{c}{NLPD $\lstates\test$} &
            \multicolumn{2}{c}{SMSE $\lstates\test$} &
            \multicolumn{2}{c}{SMSE $\obss\test$} \\
        & & mean & std. & mean & std. & mean & std.\\
        \toprule
        $\lstate$ 
& UGP & -0.90046 & 0.06743 & 0.01219 & 0.00171 & -- & -- \\
& EGP & -0.89908 & 0.06608 & 0.01224 & 0.00178 & -- & -- \\
& \cite{Opper2009} & -0.27590 & 0.06884 & 0.01249 & 0.00159 & -- & -- \\
& GP & \textbf{-0.90278} & 0.06988 & \textbf{0.01211} & 0.00160 & -- & -- \\
        \midrule
        $\lstate^3 + \lstate^2 + \lstate$ 
& UGP & \textbf{-0.23622} & 1.72609 & 0.01534 & 0.00202 & \textbf{0.02184} & 0.00525 \\
& EGP & -0.22325 & 1.76231 & \textbf{0.01518} & 0.00203 & \textbf{0.02184} & 0.00528 \\
& \cite{Opper2009} & -0.14559 & 0.04026 & 0.06733 & 0.01421 & 0.02686 & 0.00266 \\
        \midrule
        $\exp\!\brac{\lstate}$ 
& UGP & -0.75475 & 0.32376 & \textbf{0.13860} & 0.04833 & \textbf{0.03865} & 0.00403 \\
& EGP & \textbf{-0.75706} & 0.32051 & 0.13971 & 0.04842 & 0.03872 & 0.00411 \\
& \cite{Opper2009} & -0.08176 & 0.10986 & 0.17614 & 0.04845 & 0.05956 & 0.01070 \\
        \midrule
        $\sin\!\brac{\lstate}$ 
& UGP & \textbf{-0.59710} & 0.22861 & \textbf{0.03305} & 0.00840 & 0.11513 & 0.00521 \\
& EGP & -0.59705 & 0.21611 & 0.03480 & 0.00791 & \textbf{0.11478} & 0.00532 \\
& \cite{Opper2009} & -0.04363 & 0.03883 & 0.05913 & 0.01079 & 0.11890 & 0.00652 \\
        \midrule
        $\tanh\!\brac{2\lstate}$
& UGP & \textbf{0.01101} & 0.60256 & \textbf{0.15703} & 0.06077 & \textbf{0.08767} & 0.00292 \\
& EGP & 0.57403 & 1.25248 & 0.18739 & 0.07869 & 0.08874 & 0.00394 \\
& \cite{Opper2009} & 0.15743 & 0.14663 & 0.16049 & 0.04563 & 0.09434 & 0.00425 \\
        \bottomrule
    \end{tabular}
    \label{tab:toy}
\end{table}



%\vfill

\end{column}

\begin{column}{0.36\twocolwid}
\vspace{-3mm}
\begin{figure}
    \includegraphics[width=0.6\onecolwid]{fig/signdemo.png}

    \caption[]{
%    Learning the UGP with the forward model, $\nonlin{\lstate} =
  %      2\times\text{sign}\!\brac{\lstate} + \lstate^3$. This function
  %      \emph{has no derivatives},
  %      $\partial\nonlin{\lstate}\!/\partial\lstate$, at the zero crossing
  %      points. The black dots are the training data, $\cbrac{\inobs_n,
  %          \obss_n}^N_{n=1}$, used to learn the posterior parameters and
   %     hyperparameters. The blue line is the `true' latent process,
   %     $\lstate$, and the dashed red line is the predicted value of this
    %    latent process. The green line is the predicted values of the
    %    observations, $\obs\test$.
   Learning the UGP with the forward model $\nonlin{\lstate} =
     2\times\text{sign}\!\brac{\lstate} + \lstate^3$.    
       }
   \label{fig:sign}
\end{figure}

\vspace{-4mm}
\structure{\textbf{Binary classification}}

\vspace{-2mm}
\begin{table}[tb]
    \centering

    \caption[]{
    %We can also use the EGP and UGP for \emph{classification}
    %    problems. We just require a logistic sigmoid forward function,
    %    $\nonlin{\lstate} = \frac{1}{1+\exp\brac{-\lstate}}$. 
     %   Here we
      %  demonstrate the algorithms on the USPS handwritten-digits dataset for
     %   numbers `3' and `5'. We compare to other classifiers, including GP
    %    classifiers derived \emph{specifically} for this task -- our
     %   GPs only require input of a different $\nonlin{\cdot}$. 
     Perf. on USPS  for classes `3' and `5'.
    }

    \vspace{5mm}
    \small
    \begin{tabular}{r| c c}
    Algorithm & NLP $\obss\test$ & Error rate (\%) \\
    \toprule
    GP -- Laplace & 0.11528 & 2.9754  \\
    GP -- EP & 0.07522 & 2.4580  \\
    GP -- VB & 0.10891 & 3.3635  \\ 
    SVM (RBF) & 0.08055 & 2.3286  \\
    Logistic Reg. & 0.11995 & 3.6223  \\
    \midrule
    UGP & \textbf{0.07290} & \textbf{1.9405}  \\
    EGP & 0.08051 & 2.1992  \\
    %UGP & 0.06524 & 2.1992 & 1.5140 & 1.4257 \\
    %EGP & 0.12946 & 2.0699 & 3.3068 & 1.7480 \\
    \bottomrule
    \end{tabular}
    \label{tab:class}
\end{table}



\end{column}
%




\end{columns}
\end{block}


\end{column} % End of column 2.1


\end{columns} % End of the split of column 2

\end{column} % End of the second column

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % The third column

\begin{block}{Unscented Gaussian Process (UGP)}
%
\begin{figure}
    \includegraphics[height=14cm]{fig/statlin_gp}
%    \caption{Demonstration of linearising the forward model, $\nonlin{\cdot}$,
 %       using the unscented transform.}
 \caption{UGP linearises $\nonlin{\cdot}$ using the unscented transform.}
\end{figure}        

Make sigma points \cite{Julier2004} $\Sfunc_{ni}$ and $\Sobs_{ni}$,
then solving $N$ scalar linear regression problems,
\begin{equation*}
    \argmin_{\Lins_{nk}, \intcpts_{nk}} \sum^{2}_{i=0} 
        \lnorm{2}{\Sobs_{ni} - \brac{\Lins_{nk}\Sfunc_{ni} + \intcpts_{nk}}}^2
\end{equation*}
gives,
\begin{align}
    \Linmat_k &= \diag{\sbrac{\Lins_{0k}, \ldots, \Lins_{Nk}}}, \\
    \intcpt_k &= \sbrac{\bar\obss_{0} - \Lins_{0k}\pomeans_{0k}, \ldots,
        \bar\obss_{N} - \Lins_{Nk}\pomeans_{Nk}}\transpose. \nonumber
\end{align}
Here $\Lins_{nk} = \xcovs_{\obss\pomeans,nk} \pocovs_{nnk}\inv$, 
$\xcovs_{\obss\pomeans,nk}$ is the cross-covariance between $\Sfunc_{ni}$ 
and~$\Sobs_{ni}$ $\forall i$, and $\bar\obss_n = \sum_{i=0}^2 \Sw_i
\Sobs_{ni}$.

\end{block}


%----------------------------------------------------------------------------------------




\begin{block}{Learning the EGP and the UGP}

%Learning the hyperparameters, $\khypers$ in $\kernl\!\brac{\inobs,
%    \inobs'|\khypers}$, is usually an optimisation $\khypers^* =
%\argmax_{\khypers}\Fengy$,%
%\begin{itemize}
%    \item but $\pomean$ is an implicit function of $\khypers$,
%    \item so $\nabla_{\khypers}\Fengy$ also requires various derivatives of
%        $\nonlin{\cdot}$.
%\end{itemize}
%\emph{Solution}: numerical gradient methods (BOBYQA).

Variational-EM updates:
\begin{itemize}
	\item
		Optimize posterior parameters $\pomean, \pocov$
	\item
		Optimize hyperparameters and forward model's parameters 
		for fixed posterior parameters
\end{itemize}
\end{block}


%\begin{block}{Conclusion}


%\end{block}

%----------------------------------------------------------------------------------------
%	REFERENCES
%----------------------------------------------------------------------------------------

\begin{block}{References}

%\nocite{*} % Insert publications even if they are not cited in the poster
\small{\bibliographystyle{unsrt}
\bibliography{nips2014lin}\vspace{0.75in}}

\end{block}

%----------------------------------------------------------------------------------------
%	ACKNOWLEDGEMENTS
%----------------------------------------------------------------------------------------

%\setbeamercolor{block title}{fg=red,bg=white} % Change the block title color

%\begin{block}{Acknowledgements}

%\footnotesize{\rmfamily{This research was supported by the Science Industry Endowment
        %Fund (RP 04-174) Big Data Knowledge Discovery project. NICTA is funded
        %by the Australian Government through the Department of Communications
        %and the Australian Research Council through the ICT Centre of
        %Excellence Program.}} \\

%\end{block}


%----------------------------------------------------------------------------------------
%	Additional INFORMATION
%----------------------------------------------------------------------------------------

\setbeamercolor{block alerted title}{fg=white,bg=npurple!62} % Change the alert block title colors
\setbeamercolor{block alerted body}{fg=black,bg=white} % Change the alert block body colors

\vspace{-2cm}
\begin{alertblock}{Code}

Github -- \href{https://github.com/NICTA/linearizedGP}
                 {https://github.com/NICTA/linearizedGP}
\end{alertblock}

%----------------------------------------------------------------------------------------

\end{column} % End of the third column

\end{columns} % End of all the columns in the poster

\end{frame} % End of the enclosing frame

\end{document}


%%% MAYBE TODO:
% - Flesh out experiments captions
% - Make sure the linearization descriptions are 1d, they are not right now!
% - Make the figures demonstrating the linearization more reflective of the GP
%   (i.e. multiple linearization)
% - Add in posterior parameter update equations?
% - Add in prediction equations?
% - Make the equations reflect the 1-D linearizations?
% - Make A and b red to clarify their importance?
